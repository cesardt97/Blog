[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Este blog es un repositorio donde comparto ideas, códigos o artículos de opinión propia sobre temas que me parecen interesantes. Sientanse en la total libertad de contactarme si tienen una duda, desean debatir una idea o simplemente quieren conectar conmigo!"
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n\nThe aim is to explain basic economic concepts in order to understand more fluently why a credit score is important.\n\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Econometric\n\n\nGARCH\n\n\nVolatility\n\n\n\nGARCH models proposed by Bollerslev (1986) are used to analyze the historical variance of the S&P500 stock index in order to find the periods of low and high sustained…\n\n\n\nCésar Díaz Tavera\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nCésar Díaz Tavera\n\n\nAug 27, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "César Díaz",
    "section": "",
    "text": "César Iván Díaz Tavera\nMathematician, economist & curious.\nThis blog is a repository where I share ideas, codes or articles of my own opinion on topics that I find interesting. Feel free to contact me if you have a question, want to discuss an idea or just want to connect with me!\nYou can find me on LinkedIn, GitHub or reach out via email.\nRead lastest posts"
  },
  {
    "objectID": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "href": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "",
    "text": "Adverse selection is a pre-contractual asymmetric information which often leads to making bad decisions, such as doing more business with less-profitable or riskier business. In the banking set up, it may involve extending credit facilities to a borrower with high likelihood of default1.\nAsymmetric information is known as information failure and occurs when one party to an economic transaction possesses greater and material knowledge over the other party. This often manifested typically2 where the seller of a good or service possesses greater knowledge than the buyer.\nMoral hazard on the other is post-contractual asymmetric information which occurs whenever a borrower engages in behaviors that are not in the best interest of the lender for example a borrower3 diverting use a bank loan to buy lottery tickets instead of purchase of merchandise for resale, as agreed upon with the lender. It is also moral hazard if a borrower fails to repay a loan when especially in a case where the facility is unsecured knowing well that the bank stands to lose.\nFree-rider problems refers to a situation where an agent that collects information about a particular risk may be unable to prevent other agents from using that information for example a deposit insurance institution that is unable to price risk accurately.\nRational herding refers to a situation whereby an agents choosing to disregard their own information and instead react to information on the decisions taken by other agents (information externalities)."
  },
  {
    "objectID": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#implications-whatever",
    "href": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#implications-whatever",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "Implications whatever",
    "text": "Implications whatever\nInformation asymmetry between lenders and borrowers plays a key role in the field of finance and banking as one of the vital causes of financial frictions. Lenders do not know the creditworthiness of borrowers (hidden information) or cannot detect inefficient behavior of borrowers after lending (hidden action). For fear of lending to borrowers with poor credit worthiness or those that take inefficient behavior, lenders are unwilling to provide their own funds, which creates severe financial frictions. This is the problem of adverse selection (in the case of hidden information), or of moral hazard (in the case of hidden action).\nPresence of asymmetric information in the banking industry gives rise to wrong pricing patterns and selections. An asymmetric information-intensive market progressively draws away from being effective and fully competitive.\nIn credit markets, asymmetric information avoids accurate transfer of the funds, and makes it difficult to both effectively utilize the savings and efficiently finance the investments. Proper pricing and transfer to the prospective investors of the funds is only possible if the parties have full and accurate information of each other. Otherwise, investments could not be financed at appropriate conditions, and results that would adversely impact the growth of national economy might arise.\n\nPreliminares: librerias usadas, datos y análisis de los retornos\n\n\nCode\nlibrary(ggplot2)\n\nmtcars |> \n  ggplot(aes(x = mpg, y = wt)) +\n  geom_point(color = \"black\") +\n# scale_fill_manual(values = c(\"#F4F4F4\")) +\n  labs(x=\"Fuel efficiency (mpg)\", y=\"Weight (tons)\",\n       title=\"Seminal ggplot2 scatterplot example\",\n       subtitle=\"A plot that is only useful for demonstration purposes\",\n       caption=\"Source: author's elaboration\") +\n  theme_blog() \n\n\n\n\n\n\nAppendix"
  },
  {
    "objectID": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html",
    "href": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html",
    "title": "Analyzing the volatility of the Standard and Poor’s 500",
    "section": "",
    "text": "Modern portfolio theory was introduced by Nobel Prize-winning economist Henry Markowitz in 1952 with his essay ‘Portfolio Selection’ published in the Journal of Finance. Fundamentally, this theory assumes that the trade-off between return and risk1 of financial assets should not be considered on an individual basis (which was the consensus advice at the time), but that the trade-off should be considered from the portfolio as a whole. In other words, investors should focus on selecting portfolios, rather than building their portfolios from the individual selection of attractive assets. Out of the entire universe of possible portfolios, some will optimally balance risk and reward. These optimal portfolios form what Markowitz called the efficient frontier. Thus, Markowitz (1952) proposed a mathematical model for diversification.\nHowever, Henry Markowitz only considered risky assets in his theory. James Tobin (1958) extended Markowitz’s work by adding a risk-free asset to the analysis, which led to the emergence of the concepts of the ‘super-efficient portfolio’ and the ‘capital market line’. Because they are leveraged, portfolios on the capital market line can outperform portfolios on the efficient frontier (Glyn Horton, 2013). Therefore, under Tobin’s perspective, investors should diversify their portfolio by investing a \\(w\\) proportion of their wealth in a risky asset and a \\(1-w\\) proportion in risk-free assets.\nIn this paper we make use of the General Conditional Heteroskedasticity models proposed by Bollerslev (1986), to analyze in depth the historical variance of the Standard and Poor’s 500 stock index in order to find the periods of sustained low and high volatility in its returns. Thus, with the knowledge about the behavior of these volatility clusters, a theoretical exercise is proposed on the volatility predictions of the model to the asset allocation in a portfolio by means of volatility targeting, in the light of Tobin’s two-fund separation theorem."
  },
  {
    "objectID": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html#results",
    "href": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html#results",
    "title": "Analyzing the volatility of the Standard and Poor’s 500",
    "section": "Results",
    "text": "Results\n\nPreliminary: libraries used, data wrangling and analysis of returns\n\n\nCode\n# Wrangling data\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(magrittr)\n\n# Financial analysis, time series and Volatility\nlibrary(quantmod)\nlibrary(xts)\nlibrary(PerformanceAnalytics)\nlibrary(rugarch)\n\n# For tables\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nVisually, one can observe how the volatility of returns changes over time. You see that periods of large returns (positive or negative) tend to be followed by large returns, while low returns are followed by low returns. So you can see these periods of sustained high or low volatility, known as volatility clusters.\n\n\nCode\n# Upload the data\nsp500prices <- getSymbols(\"^GSPC\",auto.assign = FALSE, from = \"1989-01-03\", to = as.Date(\"2022-03-18\"))$'GSPC.Close'\ncolnames(sp500prices) <- 'SP500'\n\n# Calculate daily returns\nsp500ret <- CalculateReturns(sp500prices)\nsp500ret <- sp500ret[-1,]\nline_zero <- sp500ret\nline_zero$zero <- 0\n\n\n# Graph the price and return series of the S&P500\ngraf_returns <- plot(sp500ret, main = \"Daily returns: S&P500\")\ngraf_returns <- addSeries(line_zero[, \"zero\"], col = \"red\", on = 1)\n\npar(mfrow = c(2,1))\nplot(sp500prices, main = \"Daily prices: S&P500\")\ngraf_returns\n\n\n\n\n\n\n\nCode\n# Calculating annualized volatilities in years close to economic crises\nsd_annualized <- \n  as.data.frame(cbind(\"2008\" = sqrt(252)*sd(sp500ret[\"2008\"]), \n        \"2009\" = sqrt(252)*sd(sp500ret[\"2009\"]), \n        \"2017\" = sqrt(252)*sd(sp500ret[\"2017\"]), \n        \"2018\" = sqrt(252)*sd(sp500ret[\"2018\"]), \n        \"2019\" = sqrt(252)*sd(sp500ret[\"2019\"]), \n        \"2020\" = sqrt(252)*sd(sp500ret[\"2020\"]), \n        \"2021\" = sqrt(252)*sd(sp500ret[\"2021\"]),\n        \"total\"= sqrt(252)*sd(sp500ret, na.rm = T)))\n\nkable(sd_annualized, digits = 4, caption = \"Calculating annualized volatilities in years close to economic crises\", booktabs = T) %>% \n  kable_styling(full_width = T)\n\n\n\n\nCalculating annualized volatilities in years close to economic crises\n \n  \n    2008 \n    2009 \n    2017 \n    2018 \n    2019 \n    2020 \n    2021 \n    total \n  \n \n\n  \n    0.4097 \n    0.2729 \n    0.0669 \n    0.1705 \n    0.1247 \n    0.3443 \n    0.131 \n    0.1798 \n  \n\n\n\n\n\nVolatility fluctuations over different time periods can be observed by means of the 1-month and 3-month rolling volatility:\n\n\nCode\npar(mfrow=c(2,1))\n\n# 1-month rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"1-month rolling volatility\")\n\n# 3-months rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 3*22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"3-month rolling volatility\")\n\n\n\n\n\nAs can be seen, the volatility of S&P500 returns varies over time and may depend on past variance, so to study its dynamics a process is needed that allows the conditional variance to change over time as a function of past errors, in the manner of the autoregressive conditional heteroscedasticity (ARCH) model proposed by Engle (1982), and the subsequent generalization (GARCH) that allows for a longer memory in the model and a more flexible lag structure (Bollerslev, 1986).\nWhen developing conditional heteroscedasticity models, the specifications for the equations for the conditional mean, conditional variance and conditional error distribution should be kept in mind.\n\n\nSpecification of the conditional variance and conditional error distribution\n\n\nCode\n# Specification of the Standard GARCH model\ngarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"sGARCH\"), \n                        distribution.model = \"norm\")\n\ngarchfit <- ugarchfit(data = sp500ret, spec = garchspec)\n\n# Obtain the estimated volatility \ngarchvol <- sigma(garchfit)\nplot(garchvol, main = \"Estimated Volatility\")\n\n\n\n\n\nCode\n# Calculate the unconditional volatility and standardized returns.\nkable(sqrt(uncvariance(garchfit)), digits = 4, booktabs = T,\n      col.names =\"Unconditional inflation volatility\")\n\n\n\n\n \n  \n    Unconditional inflation volatility \n  \n \n\n  \n    0.0109 \n  \n\n\n\n\n\nCode\nstdret <- residuals(garchfit, standardize = TRUE)\n\n\n\n\nCode\n# Visual comparison of error distribution\nchart.Histogram(stdret, methods = c(\"add.normal\",\"add.density\"), \n                colorset = c(\"gray\",\"red\",\"blue\"), \n                main = \"Distribution of standardized returns vs. normal dist.\")\n\n\n\n\n\nDue to the presence of extreme events, the empirical distribution of the returns have thick tails, so the normality assumption may be inadequate and may have generated biased estimators. Considering that the distribution of the standardized returns does not follow a normal distribution, as they have excess kurtosis, thicker tails and an asymmetric effect, setting a t-Student distribution with skewness instead of a normal distribution for the specification of the conditional error distribution leads to a more realistic GARCH model.\nOn the other hand, when there is a leverage effect, negative news about returns (negative shocks) affect the variance more than positive news (positive shocks). Because of the asymmetric effect of the distribution, a gjr-GARCH model is applied to confirm a possible leverage effect. In this model, an asymmetric variance response to positive and negative news is allowed for by assigning more weight on negative news by taking \\((\\alpha + \\gamma)*\\varepsilon_t^2\\) when \\(\\varepsilon_t \\le 0\\) instead of only \\(\\alpha*\\varepsilon_t^2\\) when \\(\\varepsilon_t > 0\\). The news impact curve is a useful tool for visualizing the response of variance to surprise in returns.\n\n\nCode\n# GJR-GARCH Specification\ngjrgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                           variance.model = list(model = \"gjrGARCH\"),\n                           distribution.model = \"sstd\")\n\ngjrgarchfit <- ugarchfit(data = sp500ret, spec = gjrgarchspec)\n\n# Obtain the estimated volatility \ngjrgarchvol_mean <- fitted(gjrgarchfit)\ngjrgarchvol <- sigma(gjrgarchfit)\n \n# Compare estimated volatilities\nplotvol <- plot(abs(sp500ret), col = \"grey\", \n                main = \"Estimated returns and volatilities\")\nplotvol <- addSeries(gjrgarchvol, col = \"red\", on=1)\nplotvol <- addSeries(garchvol, col = \"blue\", on=1)\nplotvol\n\n\n\n\n\n\n\nCode\n# News Curve\nplot(gjrgarchfit, which = 12)\n\n\n\n\n\nAn asymmetric leverage effect can be seen in the figure above. From it we conclude that the conditional variance reaction is greater to past negative shocks than to past positive shocks of the same magnitude in S&P500 returns. So much so, that the market is relatively insensitive to positive shocks compared to the increase and sensitivity of volatility that accompanies negative shocks with a further decline in prices.\nModel selection based on information criteria\n\n\nCode\n# log-likelihood of both models\nlikelihood <- \n  data.frame(Garch = likelihood(garchfit), gjrGarch = likelihood(gjrgarchfit))\nrownames(likelihood) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria <- \n  data.frame(Garch = infocriteria(garchfit), gjrGarch = infocriteria(gjrgarchfit))\n\ncolnames(information_criteria) <- c(\"Garch\", \"gjrGarch\")\n\nkable(rbind(information_criteria, likelihood), booktabs = T,\n      col.names = c(\"Garch\", \"gjrGarch\"),\n      digits = 4, format = \"latex\",\n      caption = \"Information Criteria for Conditional Heteroscedasticity Models\") %>% \n  kable_styling(full_width = T, latex_options = c(\"hold_position\"))\n\n\n\n\n\nBased on the information criteria, it is concluded that the GJR-GARCH model with t-Student distribution is a more adequate and realistic model than a simple model with normal distribution.\n\n\nThe mean model\nModeling the dynamics of the conditional mean generally has a large effect on estimated returns, but only a small effect on volatility predictions. Financial econometrics theory suggests that the effect can be so minimal, that if the interest is only in the volatility dynamics, one can generally ignore the mean dynamics and assume a simple constant mean specification to save parsimony. To confirm that this is the case, we will consider an AR(1) model and a GARCH model in mean to see how the volatilities are related.\nIn the AR(1) model, the sign of the autoregressive parameter depends on the market reaction to the news \\(\\mu_t = \\mu + \\rho(R_{t-1} - \\mu)\\). A positive value of \\(\\rho\\) is consistent with the interpretation that markets underreact to news, leading to momentum in returns (above-average returns are followed by above-average returns). A negative value of \\(|rho\\) is consistent with the interpretation that markets overreact to news, leading to a reversal in returns (above-average returns are followed by below-average returns). In any case, if \\(||rho|<1\\), then deviations of returns from their long-term mean (\\(\\mu\\)) are transitory.\nSo the question arises: are the daily returns of the S&P500 characterized by momentum or a reversal effect in its AR(1) dynamics?\n\n\nCode\n# AR(1) specification of an asymmetric GARCH\nargarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)),\n                        variance.model = list(model = \"gjrGARCH\"),\n                        distribution.model = \"sstd\")\n\nargarchfit <- ugarchfit(data =  sp500ret, spec = garchspec)\n\n# Coefficients of the mean model\nkable(coef(argarchfit)[c(1:2)], digits = 6, booktabs = T, format.args = list(scientific = FALSE), col.names =\"Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional\")\n\n\n\n\n \n  \n      \n    Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional \n  \n \n\n  \n    mu \n    0.000642 \n  \n  \n    omega \n    0.000002 \n  \n\n\n\n\n\nCode\n# Mean and volatility of the AR(1) model\nar1_mean <- fitted(argarchfit)\nar1_vol <- sigma(argarchfit)\n\n\nSince the AR(1) coefficient in the mean model is negative, we find a reversal effect in terms of predicted return. After an above-average return, we expect a below-average return. And after below-average return, we expect above-average return. Also, since this coefficient is close to zero, deviations from daily returns are transitory.\nIn contrast to the use of an ARMA model for the mean, we have that a GARCH model in mean does not make use of the autocorrelation of returns. Instead, it exploits the relationship between the expected return and the variance of the return. The higher the risk in terms of variance, the higher the expected return of the investment should be. So it is a model that quantifies the trade-off between risk and reward.\n\n\nCode\n# Specification of a GARCH in mean\ngim_garchspec <- ugarchspec( \n  mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),\n  variance.model = list(model = \"gjrGARCH\"), \n  distribution.model = \"sstd\")\n\ngim_garchfit <- ugarchfit(data = sp500ret , spec = gim_garchspec)\n\n# Mean and volatility of the AR(1) model\ngim_mean <- fitted(gim_garchfit)\ngim_vol <- sigma(gim_garchfit)\n\n# Correlation between the estimated returns of the AR(1) model and the average model\nkable(cor(ar1_mean, gim_mean), \n      col.names = \"Correlación entre los retornos estimados del modelo Garch-AR(1) y el modelo Garch en media\", format = \"latex\", digits = 4, booktabs = T) %>%\n  kable_styling(full_width = T)\n\n\n\n\n\nCode\n# Correlation between the estimated returns of the mean model and the AR(1) model\nmodel_correlation <- as.data.frame(cor(merge(gjrgarchvol, ar1_vol, gim_vol)))\nrownames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\ncolnames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\n\nkable(model_correlation, booktabs = T,\n      digits = 4, format = \"latex\",\n      caption = \"Correlation between estimated volatilities for different specifications of Garch models\") %>% \n  kable_styling(full_width = T)\n\n\n\n\n\nThere is a large disagreement between the predicted returns obtained with the Garch-AR(1) and GARCH models on average, as evidenced by the low correlation. Because the mean return is close to zero for daily returns, these differences in mean prediction have little impact on volatility predictions. Their correlation is almost one. Since we are only interested in volatility dynamics, and to keep parsimony, we will estimate GARCH models with constant mean.\n\n\nExtension of the analysis of the volatility dynamics of SP500 returns.\nIn the past graphs, we observed how financial return volatility is clustered over time: periods of above-average volatility are followed by periods of below-average volatility. In the long term, it is expected that:\n\nWhen volatility is high, volatility will decline and return to its long-term average.\nWhen volatility is low, volatility will increase and return to its long-term average.\n\nIn the estimation of GARCH models we can exploit this mean-reverting behavior of volatility by volatility targeting and confirm that the long-run volatility implied by the GARCH model is equal to the sample standard deviation.\n\n\nCode\n# Specification for variance targeting\ntargarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\ntargarchfit <- ugarchfit(data = sp500ret, spec = targarchspec)\n\ntargarchvol <- sigma(targarchfit)\n\n# Compare against gjrGarch\n\n# log-likelihood of the two models\nlikelihood_2 <- \n  data.frame(gjrGarch = likelihood(gjrgarchfit), tarGarch = likelihood(targarchfit))\nrownames(likelihood_2) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria_2 <- \n  data.frame(gjrGarch = infocriteria(gjrgarchfit), tarGarch = infocriteria(targarchfit))\n\ncolnames(information_criteria_2) <- c(\"gjrGarch\", \"tarGarch\")\n\nkable(rbind(information_criteria_2, likelihood_2), booktabs = T,\n      col.names = c(\"gjrGarch\", \"Volatility targeting gjrGarch\"),\n      digits = 4, format = \"latex\",\n      caption = \"Information Criteria for GARCH models with volatility targeting\") %>% \n  kable_styling(full_width = T, latex_options = c(\"hold_position\"))\n\n\n\n\n\nCode\n# Implied long term volatility\nkable(data.frame(Garch = sqrt(uncvariance(garchfit)), \n                 tarGarch = sqrt(uncvariance(targarchfit))), \n      col.names = c(\"Simple GARCH model\", \"GJR-Garch model with volatility targeting\"), \n      format = \"latex\", digits = 4, booktabs = T,\n      caption = \"Implied long-term volatility\") %>%\n  kable_styling(full_width = T, latex_options = c(\"hold_position\"))\n\n\n\n\n\nThe models that have been studied so far lead to an in-sample volatility estimate obtained by estimating the GARCH model only once and using the full time series, which can cause bias (look-ahead bias). However, in moving window models these biases are avoided by conditioning the estimation to use only the returns available at the previous time of estimation. That is, the model would be re-estimated in each window using only the yields that are actually observable at the time of estimation.\n\n\nCode\n# Specification for garch on movable windows\nrollgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\nrollgarchfit <- ugarchroll(garchspec, \n                           data = sp500ret,\n                           n.start = 2000, refit.window = \"moving\",  refit.every = 500)\n\n# Mobile predictions\npreds <- as.data.frame(rollgarchfit)\n\n# Comparison of the estimated volatility in the in-sample model and the moving model\ngarchvolroll <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))\nvolplot <- plot(gjrgarchvol, col = \"darkgrey\", lwd = 1.5, \n                main = \"In-sample and moving window volatility forecasts\")\nvolplot <- addSeries(garchvolroll, col = \"blue\", on = 1)\nplot(volplot)\n\n\n\n\n\nTactical Asset Allocation using Target Volatility** **Targeted Volatility\nGARCH volatility predictions have a direct practical use in portfolio allocation. According to James Tobin’s two-fund separation theorem, you should invest a proportion w of your wealth in a risky portfolio and the remainder in a risk-free asset, such as a U.S. Treasury bill. If you target a portfolio with an annualized volatility of 5%, and the annualized volatility of the risky asset is w, then you should invest (0.05/w) in the risky asset, in this case, the S&P500.\n\n\nCode\n# Annualized volatility from gjrGARCH moving window model with \n# asymmetric t-Student distribution\nestimated_annualized_volatility <- sqrt(252)*garchvolroll\n\n# Calculate the weights assigned to the risky asset with a target volatility of 5% per year. \nweight <- 0.05 / estimated_annualized_volatility\n\n# Compare the annualized volatility with the portfolio's weights\nplot(merge(estimated_annualized_volatility, weight), multi.panel = TRUE,\n     main = \"Annualized vol. and the risk asset allocation weights\")\n\n\n\n\n\nIn the above chart it is easy to observe the dynamics of asset allocation within a portfolio. In our portfolio composed of U.S. treasury bills and the S&P500, since treasury bills are risk-free, the question that arises is what weight should be assigned to the S&P500 within the portfolio given the risk exposure constraint we impose on ourselves. By March 2022, the model recommends a portfolio with approximately 20% exposure to the S&P500 and 80% in risk-free assets with a target volatility of 5%.\nBrief application of a VaR - Value at Risk.\nThe value-at-risk charts show substantial temporal variation in downside risk. This time variation is primarily due to the time variation in volatility.\n\n\nCode\n# 5% Value at Risk\ngarchVaR <- quantile(rollgarchfit, probs = 0.05)\n\n# Volatility for Value at Risk\ngarchvolroll_var <- xts(preds$Sigma, order.by = time(garchVaR))\n\n# Visual analysis of movements\ngarchplot <- plot(garchvolroll_var, ylim = c(-0.1, 0.1), \n     main = \"Daily volatility and 5% VaR\")\ngarchplot <- addSeries(garchVaR, on = 1, col = \"blue\")\nplot(garchplot, \n     main = \"Daily volatility and 5% VaR\")\n\n\n\n\n\nThe high co-movement between the two series is notorious. The intuition is that, if volatility soars, there is a risk of losing more money. That is why the value at risk also becomes more extreme. In March 2022, the value at risk of this portfolio is 1.95%, or almost $20 for every $1,000 invested in the risky asset.\nUsing the weights assigned in the previous exercise, if one were to invest $100,000 today: $20,000 would be invested in the S&P500 and $80,000 in U.S. Treasury bills. In aggregate, the portfolio would have a Value at Risk of $390.5. An amount exposed to risk of only 0.391% of invested capital."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{díaztavera2022,\n  author = {César Díaz Tavera},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2022-08-27},\n  url = {https://cesardt97.github.io/posts/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCésar Díaz Tavera. 2022. “Welcome To My Blog.” August 27,\n2022. https://cesardt97.github.io/posts/welcome."
  },
  {
    "objectID": "serie-credit-score.html",
    "href": "serie-credit-score.html",
    "title": "Credit Score Series",
    "section": "",
    "text": "Credit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n \n\n\n\n\nSep 5, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  }
]