[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I studied mathematics with a concentration in statistics and actuarial science, and finished my masters degree in economics with concentration in financial economics in 2022. I’m currently working at the Central Bank of the Dominican Republic as an Economist, where I am responsible for the short-term analysis of the national and international economy to shed light on market conditions for policymakers. This is done through economic research and the development of economic indicators with econometric and machine learning models. I have strong problem-solving skills and I’m really passionate about data-centric solutions, data visualization and statistical modelling. I mainly work in the programming language R, but I’m also versed in Python and SQL."
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nCredit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n\nThe aim is to explain basic economic concepts in order to understand more fluently why a credit score is important.\n\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\nSep 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinancial Econometric\n\n\nGARCH\n\n\nVolatility\n\n\n\nGARCH models proposed by Bollerslev (1986) are used to analyze the historical variance of the S&P500 stock index in order to find the periods of low and high sustained…\n\n\n\nCésar Díaz Tavera\n\n\nAug 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\nCésar Díaz Tavera\n\n\nAug 27, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "César Díaz",
    "section": "",
    "text": "César Iván Díaz Tavera\nMathematician, economist & curious.\nThis blog is a repository where I share ideas, codes or articles of my own opinion on topics that I find interesting. Feel free to contact me if you have a question, want to discuss an idea or just want to connect with me!\nYou can find me on LinkedIn, GitHub or reach out via email.\nRead lastest posts"
  },
  {
    "objectID": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "href": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "",
    "text": "The sustainable and healthy development of a country’s financial system and banking sector is of utmost importance for economic growth. As banks are the main source of credit, they provide economic agents with funds to consume and invest, adding dynamism to the economic activity. However, the banking sector is full of risks. That is why risk management is an area of extreme importance and has remained a topic of major interest since the financial crisis of 2007-2008. Today, risk management is one of the central functions of financial institutions.\nBecause of its importance, Juan Quiñonez and I have decided to carry out a brief series of studies where we consider a type of risk and, specifically, one of the ways in which it can be mitigated. We refer to credit risk and credit scores, respectively. This study series on credit score in banking will cover the following:\n\nIntroduction to the study series: key concepts to understand credit scoring\nThe Importance of a Credit Score\nMethodologies for Credit Score development\n\nLogit model\n\nOut of sample evaluation\nKolmogorov–Smirnov test\n\nMachine learning models\nLogit benchmark vs ML models\n\n\nThe goal of the study series is to better understand what a credit score is, why it is important, and how it is constructed. Explicitly, after briefly laying out a theoretical background on its importance, we aim to delved into the statistical techniques that are appropriate to construct a good credit score system. This article is the first in the series of studies and reviews the key economic concepts involved in the underlying problem that score credits try to mitigate."
  },
  {
    "objectID": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#definition-of-key-concepts",
    "href": "posts/series-sc/intro-to-the-study-serie/09-05-2022_intro_study_serie.html#definition-of-key-concepts",
    "title": "Introduction to the study serie: key concepts to understand credit scoring",
    "section": "Definition of key concepts",
    "text": "Definition of key concepts\nInformation asymmetry cause the healthy functioning of markets to disappear.\nInformation is the basis of decision making. Whenever we want to do a transaction of any kind1 we try to collect as much information as possible in order to gain bargain power, or simply to be sure that the transaction is beneficial. Information asymmetry refers to a situation of imbalance in which, in a transaction of any kind, one of the interested parties has more or better information than the other, which creates inefficiency and even market failures in some cases. Popular examples of information asymmetry are moral hazard and adverse selection.\nOn the other hand, adverse selection is a pre-contractual problem that leads to bad decision2. Since Akerlof (1978) seminal article ‘the market for lemons’3, where he explains the concept and its implications in market dynamics, adverse selection has come to be seen as a fundamental cause of market failure. Housing, insurance, credit markets and others suffer constantly from this problem. Put simply, adverse selection is the problem of correctly distinguish between a good and a bad product4 when there is information asymmetry in an economic transaction. This problem arises because sometimes it is very costly/difficult to be fully informed about the quality or features of a product. In markets where this problem is consistent, the overall quality of the products at sell in the market significantly declines5. In the banking industry, adverse selection is common as an issue concerning the provision of credit, since the intermediary can’t know for sure who has a high probability of default.\nMoral hazard is a post-contractual information asymmetry problem that occurs when one of the parties to a contract takes actions that represent additional risk (and thus are detrimental) to the other party, within the framework of what had been agreed. For example, if a borrower takes out a loan with the idea of starting a retail business with which he would pay the installment, but uses the money to travel, his probability of default increases, hence increasing in the lender’s credit risk.\nThus, these problems of information asymmetry permeate the financial markets with risk. Risk can be defined as the negative consequences6 that arise due to uncertainty. In other words, risk can be seen as the potential loss resulting from the interaction with uncertainty, an interaction that occurs according to the degree of exposure7 to chance. Uncertainty cannot be changed, but the degree of exposure to it can Bessis (2011).\nAmong the risks that exist in banking, it is up to us to discuss credit risk. Credit risk is the expected loss resulting from a borrower’s incapability to meet its obligation. So, it refers to the amount lost when a borrower fails to make a payment, which includes a principal, interest and collection costs. Credit risk can be understood as the multiplication of its components:\n\\[\nCredit\\ risk=(default\\ probability)*(exposure\\ at\\ default)*(loss\\ rate\\ given\\ default)\n\\] Where default probability is the likelihood of borrowers’ no being able to comply with their obligations. Exposure is the amount to be lost if the borrower where to default. Loss rate given default is the percentage of amount lost after collection efforts.\nLet’s analyze this with an example: Let’s say Juan wants to buy a car for $100,000 and the bank will fund 90% of the purchase, so the loan amount is equal to $90,000. Up to this day, Juan has repaid $10,000, so the outstanding balance is $80,000. If Juan defaults, the exposure at default would be $80,000. Let’s suppose there is evidence that the probability of default is 30%. If the borrower defaults, the bank can sell the car immediately for $70,000. Then, the remaining loss would be $10,000 and the loss given default would be \\(\\$10,000/\\$90,000 = 11.11%\\). So if the bank where to calculate its expected loss at this very moment, it would be \\(30\\% * 11.11\\% * \\$90,000 = \\$3,000\\).\nFinally, I would like you to take away the idea of why effective supervision is important for the banking sector. Gündüz (2020) : The main purpose of surveillance and supervision is to ensure that banks retain sufficient capital against the risks they bear and to ensure that they operate in an environment where reliable conditions are created. Effective surveillance and supervision in banking plays a critical role in ensuring stability in the financial system of every country. It provides the benefits in free market conditions and in the implementation of effective macroeconomic policies.\nIn the next article in this series of studies we will delve into how these concepts are interrelated, their implications for the banking industry, and why a good credit scoring system is important for financial institutions."
  },
  {
    "objectID": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html",
    "href": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html",
    "title": "Analyzing the volatility of the Standard and Poor’s 500",
    "section": "",
    "text": "Modern portfolio theory was introduced by Nobel Prize-winning economist Henry Markowitz in 1952 with his essay ‘Portfolio Selection’ published in the Journal of Finance. Fundamentally, this theory assumes that the trade-off between return and risk1 of financial assets should not be considered on an individual basis (which was the consensus advice at the time), but that the trade-off should be considered from the portfolio as a whole. In other words, investors should focus on selecting portfolios, rather than building their portfolios from the individual selection of attractive assets. Out of the entire universe of possible portfolios, some will optimally balance risk and reward. These optimal portfolios form what Markowitz called the efficient frontier. Thus, Markowitz (1952) proposed a mathematical model for diversification.\nHowever, Henry Markowitz only considered risky assets in his theory. James Tobin (1958) extended Markowitz’s work by adding a risk-free asset to the analysis, which led to the emergence of the concepts of the ‘super-efficient portfolio’ and the ‘capital market line’. Because they are leveraged, portfolios on the capital market line can outperform portfolios on the efficient frontier (Glyn Horton, 2013). Therefore, under Tobin’s perspective, investors should diversify their portfolio by investing a \\(w\\) proportion of their wealth in a risky asset and a \\(1-w\\) proportion in risk-free assets.\nIn this paper we make use of the General Conditional Heteroskedasticity models proposed by Bollerslev (1986), to analyze in depth the historical variance of the Standard and Poor’s 500 stock index in order to find the periods of sustained low and high volatility in its returns. Thus, with the knowledge about the behavior of these volatility clusters, a theoretical exercise is proposed on the volatility predictions of the model to the asset allocation in a portfolio by means of volatility targeting, in the light of Tobin’s two-fund separation theorem."
  },
  {
    "objectID": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html#results",
    "href": "posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html#results",
    "title": "Analyzing the volatility of the Standard and Poor’s 500",
    "section": "Results",
    "text": "Results\n\nPreliminary: libraries used, data wrangling and analysis of returns\n\n\nCode\n# Wrangling data\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(magrittr)\n\n# Financial analysis, time series and Volatility\nlibrary(quantmod)\nlibrary(xts)\nlibrary(PerformanceAnalytics)\nlibrary(rugarch)\n\n# For tables\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nVisually, one can observe how the volatility of returns changes over time. You see that periods of large returns (positive or negative) tend to be followed by large returns, while low returns are followed by low returns. So you can see these periods of sustained high or low volatility, known as volatility clusters.\n\n\nCode\n# Upload the data\nsp500prices <- getSymbols(\"^GSPC\",auto.assign = FALSE, from = \"1989-01-03\", to = as.Date(\"2022-03-18\"))$'GSPC.Close'\ncolnames(sp500prices) <- 'SP500'\n\n# Calculate daily returns\nsp500ret <- CalculateReturns(sp500prices)\nsp500ret <- sp500ret[-1,]\nline_zero <- sp500ret\nline_zero$zero <- 0\n\n\n# Graph the price and return series of the S&P500\ngraf_returns <- plot(sp500ret, main = \"Daily returns: S&P500\")\ngraf_returns <- addSeries(line_zero[, \"zero\"], col = \"red\", on = 1)\n\npar(mfrow = c(2,1))\nplot(sp500prices, main = \"Daily prices: S&P500\")\ngraf_returns\n\n\n\n\n\n\n\nCode\n# Calculating annualized volatilities in years close to economic crises\nsd_annualized <- \n  as.data.frame(cbind(\"2008\" = sqrt(252)*sd(sp500ret[\"2008\"]), \n        \"2009\" = sqrt(252)*sd(sp500ret[\"2009\"]), \n        \"2017\" = sqrt(252)*sd(sp500ret[\"2017\"]), \n        \"2018\" = sqrt(252)*sd(sp500ret[\"2018\"]), \n        \"2019\" = sqrt(252)*sd(sp500ret[\"2019\"]), \n        \"2020\" = sqrt(252)*sd(sp500ret[\"2020\"]), \n        \"2021\" = sqrt(252)*sd(sp500ret[\"2021\"]),\n        \"total\"= sqrt(252)*sd(sp500ret, na.rm = T)))\n\nkable(sd_annualized, digits = 4, caption = \"Calculating annualized volatilities in years close to economic crises\", booktabs = T) %>% \n  kable_styling(full_width = T)\n\n\n\n\nCalculating annualized volatilities in years close to economic crises\n \n  \n    2008 \n    2009 \n    2017 \n    2018 \n    2019 \n    2020 \n    2021 \n    total \n  \n \n\n  \n    0.4097 \n    0.2729 \n    0.0669 \n    0.1705 \n    0.1247 \n    0.3443 \n    0.131 \n    0.1798 \n  \n\n\n\n\n\nVolatility fluctuations over different time periods can be observed by means of the 1-month and 3-month rolling volatility:\n\n\nCode\npar(mfrow=c(2,1))\n\n# 1-month rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"1-month rolling volatility\")\n\n# 3-months rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 3*22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"3-month rolling volatility\")\n\n\n\n\n\nAs can be seen, the volatility of S&P500 returns varies over time and may depend on past variance, so to study its dynamics a process is needed that allows the conditional variance to change over time as a function of past errors, in the manner of the autoregressive conditional heteroscedasticity (ARCH) model proposed by Engle (1982), and the subsequent generalization (GARCH) that allows for a longer memory in the model and a more flexible lag structure (Bollerslev, 1986).\nWhen developing conditional heteroscedasticity models, the specifications for the equations for the conditional mean, conditional variance and conditional error distribution should be kept in mind.\n\n\nSpecification of the conditional variance and conditional error distribution\n\n\nCode\n# Specification of the Standard GARCH model\ngarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"sGARCH\"), \n                        distribution.model = \"norm\")\n\ngarchfit <- ugarchfit(data = sp500ret, spec = garchspec)\n\n# Obtain the estimated volatility \ngarchvol <- sigma(garchfit)\nplot(garchvol, main = \"Estimated Volatility\")\n\n\n\n\n\nCode\n# Calculate the unconditional volatility and standardized returns.\nkable(sqrt(uncvariance(garchfit)), digits = 4, booktabs = T,\n      col.names =\"Unconditional inflation volatility\")\n\n\n\n\n \n  \n    Unconditional inflation volatility \n  \n \n\n  \n    0.0109 \n  \n\n\n\n\n\n\n\nCode\n# Visual comparison of error distribution\nstdret <- residuals(garchfit, standardize = TRUE)\n\nchart.Histogram(stdret, methods = c(\"add.normal\",\"add.density\"), \n                colorset = c(\"gray\",\"red\",\"blue\"), \n                main = \"Distribution of standardized returns vs. normal dist.\")\n\n\n\n\n\nDue to the presence of extreme events, the empirical distribution of the returns have thick tails, so the normality assumption may be inadequate and may have generated biased estimators. Considering that the distribution of the standardized returns does not follow a normal distribution, as they have excess kurtosis, thicker tails and an asymmetric effect, setting a t-Student distribution with skewness instead of a normal distribution for the specification of the conditional error distribution leads to a more realistic GARCH model.\nOn the other hand, when there is a leverage effect, negative news about returns (negative shocks) affect the variance more than positive news (positive shocks). Because of the asymmetric effect of the distribution, a gjr-GARCH model is applied to confirm a possible leverage effect. In this model, an asymmetric variance response to positive and negative news is allowed for by assigning more weight on negative news by taking \\((\\alpha + \\gamma)*\\varepsilon_t^2\\) when \\(\\varepsilon_t \\le 0\\) instead of only \\(\\alpha*\\varepsilon_t^2\\) when \\(\\varepsilon_t > 0\\). The news impact curve is a useful tool for visualizing the response of variance to surprise in returns.\n\n\nCode\n# GJR-GARCH Specification\ngjrgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                           variance.model = list(model = \"gjrGARCH\"),\n                           distribution.model = \"sstd\")\n\ngjrgarchfit <- ugarchfit(data = sp500ret, spec = gjrgarchspec)\n\n# Obtain the estimated volatility \ngjrgarchvol_mean <- fitted(gjrgarchfit)\ngjrgarchvol <- sigma(gjrgarchfit)\n \n# Compare estimated volatilities\nplotvol <- plot(abs(sp500ret), col = \"grey\", \n                main = \"Estimated returns and volatilities\")\nplotvol <- addSeries(gjrgarchvol, col = \"red\", on=1)\nplotvol <- addSeries(garchvol, col = \"blue\", on=1)\nplotvol\n\n\n\n\n\n\n\nCode\n# News Curve\nplot(gjrgarchfit, which = 12)\n\n\n\n\n\nAn asymmetric leverage effect can be seen in the figure above. From it we conclude that the conditional variance reaction is greater to past negative shocks than to past positive shocks of the same magnitude in S&P500 returns. So much so, that the market is relatively insensitive to positive shocks compared to the increase and sensitivity of volatility that accompanies negative shocks with a further decline in prices.\nModel selection based on information criteria\n\n\nCode\n# log-likelihood of both models\nlikelihood <- \n  data.frame(Garch = likelihood(garchfit), gjrGarch = likelihood(gjrgarchfit))\nrownames(likelihood) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria <- \n  data.frame(Garch = infocriteria(garchfit), gjrGarch = infocriteria(gjrgarchfit))\n\ncolnames(information_criteria) <- c(\"Garch\", \"gjrGarch\")\n\nkable(rbind(information_criteria, likelihood), booktabs = T,\n      col.names = c(\"Garch\", \"gjrGarch\"),\n      digits = 4,\n      caption = \"Information Criteria for Conditional Heteroscedasticity Models\") %>% \n  kable_styling(full_width = T)\n\n\n\n\nInformation Criteria for Conditional Heteroscedasticity Models\n \n  \n      \n    Garch \n    gjrGarch \n  \n \n\n  \n    Akaike \n    -6.5656 \n    -6.6531 \n  \n  \n    Bayes \n    -6.5623 \n    -6.6472 \n  \n  \n    Shibata \n    -6.5656 \n    -6.6531 \n  \n  \n    Hannan-Quinn \n    -6.5645 \n    -6.6511 \n  \n  \n    Log-Likelihood \n    27471.3350 \n    27840.3342 \n  \n\n\n\n\n\nBased on the information criteria, it is concluded that the GJR-GARCH model with t-Student distribution is a more adequate and realistic model than a simple model with normal distribution.\n\n\nThe mean model\nModeling the dynamics of the conditional mean generally has a large effect on estimated returns, but only a small effect on volatility predictions. Financial econometrics theory suggests that the effect can be so minimal, that if the interest is only in the volatility dynamics, one can generally ignore the mean dynamics and assume a simple constant mean specification to save parsimony. To confirm that this is the case, we will consider an AR(1) model and a GARCH model in mean to see how the volatilities are related.\nIn the AR(1) model, the sign of the autoregressive parameter depends on the market reaction to the news \\(\\mu_t = \\mu + \\rho(R_{t-1} - \\mu)\\). A positive value of \\(\\rho\\) is consistent with the interpretation that markets underreact to news, leading to momentum in returns (above-average returns are followed by above-average returns). A negative value of \\(|rho\\) is consistent with the interpretation that markets overreact to news, leading to a reversal in returns (above-average returns are followed by below-average returns). In any case, if \\(||rho|<1\\), then deviations of returns from their long-term mean (\\(\\mu\\)) are transitory.\nSo the question arises: are the daily returns of the S&P500 characterized by momentum or a reversal effect in its AR(1) dynamics?\n\n\nCode\n# AR(1) specification of an asymmetric GARCH\nargarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)),\n                        variance.model = list(model = \"gjrGARCH\"),\n                        distribution.model = \"sstd\")\n\nargarchfit <- ugarchfit(data =  sp500ret, spec = garchspec)\n\n# Mean and volatility of the AR(1) model\nar1_mean <- fitted(argarchfit)\nar1_vol <- sigma(argarchfit)\n\n# Coefficients of the mean model\nkable(coef(argarchfit)[c(1:2)], digits = 6, booktabs = T, format.args = list(scientific = FALSE), col.names =\"Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional\") %>% \n  kable_styling(full_width = T)\n\n\n\n\n \n  \n      \n    Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional \n  \n \n\n  \n    mu \n    0.000642 \n  \n  \n    omega \n    0.000002 \n  \n\n\n\n\n\nSince the AR(1) coefficient in the mean model is negative, we find a reversal effect in terms of predicted return. After an above-average return, we expect a below-average return. And after below-average return, we expect above-average return. Also, since this coefficient is close to zero, deviations from daily returns are transitory.\nIn contrast to the use of an ARMA model for the mean, we have that a GARCH model in mean does not make use of the autocorrelation of returns. Instead, it exploits the relationship between the expected return and the variance of the return. The higher the risk in terms of variance, the higher the expected return of the investment should be. So it is a model that quantifies the trade-off between risk and reward.\n\n\nCode\n# Specification of a GARCH in mean\ngim_garchspec <- ugarchspec( \n  mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),\n  variance.model = list(model = \"gjrGARCH\"), \n  distribution.model = \"sstd\")\n\ngim_garchfit <- ugarchfit(data = sp500ret , spec = gim_garchspec)\n\n# Mean and volatility of the AR(1) model\ngim_mean <- fitted(gim_garchfit)\ngim_vol <- sigma(gim_garchfit)\n\n# Correlation between the estimated returns of the AR(1) model and the average model\nkable(cor(ar1_mean, gim_mean), \n      col.names = \"Correlación entre los retornos estimados del modelo Garch-AR(1) y el modelo Garch en media\", digits = 4, booktabs = T) %>%\n  kable_styling(full_width = T)\n\n\n\n\n \n  \n    Correlación entre los retornos estimados del modelo Garch-AR(1) y el modelo Garch en media \n  \n \n\n  \n    0.4406 \n  \n\n\n\n\n\nCode\n# Correlation between the estimated returns of the mean model and the AR(1) model\nmodel_correlation <- as.data.frame(cor(merge(gjrgarchvol, ar1_vol, gim_vol)))\nrownames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\ncolnames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\n\nkable(model_correlation, booktabs = T,\n      digits = 4,\n      caption = \"Correlation between estimated volatilities for different specifications of Garch models\") %>% \n  kable_styling(full_width = T)\n\n\n\n\nCorrelation between estimated volatilities for different specifications of Garch models\n \n  \n      \n    gjrGarch \n    AR(1) \n    Garch in mean \n  \n \n\n  \n    gjrGarch \n    1.0000 \n    0.9678 \n    0.9993 \n  \n  \n    AR(1) \n    0.9678 \n    1.0000 \n    0.9696 \n  \n  \n    Garch in mean \n    0.9993 \n    0.9696 \n    1.0000 \n  \n\n\n\n\n\nThere is a large disagreement between the predicted returns obtained with the Garch-AR(1) and GARCH models on average, as evidenced by the low correlation. Because the mean return is close to zero for daily returns, these differences in mean prediction have little impact on volatility predictions. Their correlation is almost one. Since we are only interested in volatility dynamics, and to keep parsimony, we will estimate GARCH models with constant mean.\n\n\nExtension of the analysis of the volatility dynamics of SP500 returns.\nIn the past graphs, we observed how financial return volatility is clustered over time: periods of above-average volatility are followed by periods of below-average volatility. In the long term, it is expected that:\n\nWhen volatility is high, volatility will decline and return to its long-term average.\nWhen volatility is low, volatility will increase and return to its long-term average.\n\nIn the estimation of GARCH models we can exploit this mean-reverting behavior of volatility by volatility targeting and confirm that the long-run volatility implied by the GARCH model is equal to the sample standard deviation.\n\n\nCode\n# Specification for variance targeting\ntargarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\ntargarchfit <- ugarchfit(data = sp500ret, spec = targarchspec)\n\ntargarchvol <- sigma(targarchfit)\n\n# Compare against gjrGarch\n\n# log-likelihood of the two models\nlikelihood_2 <- \n  data.frame(gjrGarch = likelihood(gjrgarchfit), tarGarch = likelihood(targarchfit))\nrownames(likelihood_2) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria_2 <- \n  data.frame(gjrGarch = infocriteria(gjrgarchfit), tarGarch = infocriteria(targarchfit))\n\ncolnames(information_criteria_2) <- c(\"gjrGarch\", \"tarGarch\")\n\nkable(rbind(information_criteria_2, likelihood_2), booktabs = T,\n      col.names = c(\"gjrGarch\", \"Volatility targeting gjrGarch\"),\n      digits = 4,\n      caption = \"Information Criteria for GARCH models with volatility targeting\") %>% \n  kable_styling(full_width = T)\n\n\n\n\nInformation Criteria for GARCH models with volatility targeting\n \n  \n      \n    gjrGarch \n    Volatility targeting gjrGarch \n  \n \n\n  \n    Akaike \n    -6.6531 \n    -6.6532 \n  \n  \n    Bayes \n    -6.6472 \n    -6.6482 \n  \n  \n    Shibata \n    -6.6531 \n    -6.6532 \n  \n  \n    Hannan-Quinn \n    -6.6511 \n    -6.6515 \n  \n  \n    Log-Likelihood \n    27840.3342 \n    27839.8445 \n  \n\n\n\n\n\nCode\n# Implied long term volatility\nkable(data.frame(Garch = sqrt(uncvariance(garchfit)), \n                 tarGarch = sqrt(uncvariance(targarchfit))), \n      col.names = c(\"Simple GARCH model\", \"GJR-Garch model with volatility targeting\"), \n      digits = 4, booktabs = T,\n      caption = \"Implied long-term volatility\") %>%\n  kable_styling(full_width = T)\n\n\n\n\nImplied long-term volatility\n \n  \n    Simple GARCH model \n    GJR-Garch model with volatility targeting \n  \n \n\n  \n    0.0109 \n    0.0113 \n  \n\n\n\n\n\nThe models that have been studied so far lead to an in-sample volatility estimate obtained by estimating the GARCH model only once and using the full time series, which can cause bias (look-ahead bias). However, in moving window models these biases are avoided by conditioning the estimation to use only the returns available at the previous time of estimation. That is, the model would be re-estimated in each window using only the yields that are actually observable at the time of estimation.\n\n\nCode\n# Specification for garch on movable windows\nrollgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\nrollgarchfit <- ugarchroll(garchspec, \n                           data = sp500ret,\n                           n.start = 2000, refit.window = \"moving\",  refit.every = 500)\n\n# Mobile predictions\npreds <- as.data.frame(rollgarchfit)\n\n# Comparison of the estimated volatility in the in-sample model and the moving model\ngarchvolroll <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))\nvolplot <- plot(gjrgarchvol, col = \"darkgrey\", lwd = 1.5, \n                main = \"In-sample and moving window volatility forecasts\")\nvolplot <- addSeries(garchvolroll, col = \"blue\", on = 1)\nplot(volplot)\n\n\n\n\n\nTactical Asset Allocation using Target Volatility** **Targeted Volatility\nGARCH volatility predictions have a direct practical use in portfolio allocation. According to James Tobin’s two-fund separation theorem, you should invest a proportion w of your wealth in a risky portfolio and the remainder in a risk-free asset, such as a U.S. Treasury bill. If you target a portfolio with an annualized volatility of 5%, and the annualized volatility of the risky asset is w, then you should invest (0.05/w) in the risky asset, in this case, the S&P500.\n\n\nCode\n# Annualized volatility from gjrGARCH moving window model with \n# asymmetric t-Student distribution\nestimated_annualized_volatility <- sqrt(252)*garchvolroll\n\n# Calculate the weights assigned to the risky asset with a target volatility of 5% per year. \nweight <- 0.05 / estimated_annualized_volatility\n\n# Compare the annualized volatility with the portfolio's weights\nplot(merge(estimated_annualized_volatility, weight), multi.panel = TRUE,\n     main = \"Annualized vol. and the risk asset allocation weights\")\n\n\n\n\n\nIn the above chart it is easy to observe the dynamics of asset allocation within a portfolio. In our portfolio composed of U.S. treasury bills and the S&P500, since treasury bills are risk-free, the question that arises is what weight should be assigned to the S&P500 within the portfolio given the risk exposure constraint we impose on ourselves. By March 2022, the model recommends a portfolio with approximately 20% exposure to the S&P500 and 80% in risk-free assets with a target volatility of 5%.\nBrief application of a VaR - Value at Risk.\nThe value-at-risk charts show substantial temporal variation in downside risk. This time variation is primarily due to the time variation in volatility.\n\n\nCode\n# 5% Value at Risk\ngarchVaR <- quantile(rollgarchfit, probs = 0.05)\n\n# Volatility for Value at Risk\ngarchvolroll_var <- xts(preds$Sigma, order.by = time(garchVaR))\n\n# Visual analysis of movements\ngarchplot <- plot(garchvolroll_var, ylim = c(-0.1, 0.1), \n     main = \"Daily volatility and 5% VaR\")\ngarchplot <- addSeries(garchVaR, on = 1, col = \"blue\")\nplot(garchplot, \n     main = \"Daily volatility and 5% VaR\")\n\n\n\n\n\nThe high co-movement between the two series is notorious. The intuition is that, if volatility soars, there is a risk of losing more money. That is why the value at risk also becomes more extreme. In March 2022, the value at risk of this portfolio is 1.95%, or almost $20 for every $1,000 invested in the risky asset.\nUsing the weights assigned in the previous exercise, if one were to invest $100,000 today: $20,000 would be invested in the S&P500 and $80,000 in U.S. Treasury bills. In aggregate, the portfolio would have a Value at Risk of $390.5. An amount exposed to risk of only 0.391% of invested capital."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{díaztavera2022,\n  author = {César Díaz Tavera},\n  title = {Welcome {To} {My} {Blog}},\n  date = {2022-08-27},\n  url = {https://cesardt97.github.io/posts/welcome},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nCésar Díaz Tavera. 2022. “Welcome To My Blog.” August 27,\n2022. https://cesardt97.github.io/posts/welcome."
  },
  {
    "objectID": "serie-credit-score.html",
    "href": "serie-credit-score.html",
    "title": "Credit Score Series",
    "section": "",
    "text": "Credit Score\n\n\nEconomics\n\n\nInformation Asymmetry\n\n\nBanking\n\n\n\n\nThe aim is to explain basic economic concepts in order to understand more fluently why a credit score is important.\n\n\n\n\n\n\nSep 5, 2022\n\n\nCésar Díaz Tavera, Juan Quiñonez Wu\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  }
]