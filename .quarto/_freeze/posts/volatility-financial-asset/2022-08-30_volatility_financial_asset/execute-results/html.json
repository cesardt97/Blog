{
  "hash": "1052b38ad3c0f492087aa023c472aeb4",
  "result": {
    "markdown": "---\ntitle: \"Analyzing the volatility of the Standard and Poor's 500\"\ndescription: |\n  GARCH models proposed by Bollerslev (1986) are used to analyze the historical variance of the S&P500 stock index in order to find the periods of low and high sustained volatility in its returns. Likewise, a theoretical exercise on volatility predictions and asset allocation in a portfolio by means of volatility targets is developed, in the light of Tobin's two-fund separation theorem.\n\nauthor: \"César Díaz Tavera\"\ndate: 2022-08-30\ncategories: [Financial Econometric, GARCH, Volatility]\nimage: \"financial-econometrics.png\"\ncitation:\n  url: https://cesardt97.github.io/Blog/posts/volatility-financial-asset/2022-08-30_volatility_financial_asset.html\nparams:\n  slug: volatily-financial-asset\n  date: 2022-08-30\n---\n\n\n## **Introduction**\n\nModern portfolio theory was introduced by Nobel Prize-winning economist Henry Markowitz in 1952 with his essay 'Portfolio Selection' published in the *Journal of Finance*. Fundamentally, this theory assumes that the trade-off between return and risk[^1] of financial assets should not be considered on an individual basis (which was the consensus advice at the time), but that the trade-off should be considered from the portfolio as a whole. In other words, investors should focus on selecting portfolios, rather than building their portfolios from the individual selection of attractive assets. Out of the entire universe of possible portfolios, some will optimally balance risk and reward. These optimal portfolios form what Markowitz called the *efficient frontier*. Thus, Markowitz (1952) proposed a mathematical model for **diversification**.\n\n[^1]: To analyze these measures of profitability and risk, Markowitz proposed to take as *proxies* for them the expected return and variance, respectively.\n\nHowever, Henry Markowitz only considered risky assets in his theory. James Tobin (1958) extended Markowitz's work by adding a risk-free asset to the analysis, which led to the emergence of the concepts of the 'super-efficient portfolio' and the 'capital market line'. Because they are leveraged, portfolios on the capital market line can outperform portfolios on the efficient frontier (Glyn Horton, 2013). Therefore, under Tobin's perspective, investors should diversify their portfolio by investing a $w$ proportion of their wealth in a risky asset and a $1-w$ proportion in risk-free assets.\n\nIn this paper we make use of the General Conditional Heteroskedasticity models proposed by Bollerslev (1986), to analyze in depth the historical variance of the *Standard and Poor's 500* stock index in order to find the periods of sustained low and high volatility in its returns. Thus, with the knowledge about the behavior of these volatility clusters, a theoretical exercise is proposed on the volatility predictions of the model to the asset allocation in a portfolio by means of volatility targeting, in the light of Tobin's two-fund separation theorem.\n\n## Results\n\n### Preliminary: libraries used, data wrangling and analysis of returns\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Wrangling data\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(magrittr)\n\n# Financial analysis, time series and Volatility\nlibrary(quantmod)\nlibrary(xts)\nlibrary(PerformanceAnalytics)\nlibrary(rugarch)\n\n# For tables\nlibrary(knitr)\nlibrary(kableExtra)\n```\n:::\n\n\nVisually, one can observe how the volatility of returns changes over time. You see that periods of large returns (positive or negative) tend to be followed by large returns, while low returns are followed by low returns. So you can see these periods of sustained high or low volatility, known as volatility clusters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Upload the data\nsp500prices <- getSymbols(\"^GSPC\",auto.assign = FALSE, from = \"1989-01-03\", to = as.Date(\"2022-03-18\"))$'GSPC.Close'\ncolnames(sp500prices) <- 'SP500'\n\n# Calculate daily returns\nsp500ret <- CalculateReturns(sp500prices)\nsp500ret <- sp500ret[-1,]\nline_zero <- sp500ret\nline_zero$zero <- 0\n\n\n# Graph the price and return series of the S&P500\ngraf_returns <- plot(sp500ret, main = \"Daily returns: S&P500\")\ngraf_returns <- addSeries(line_zero[, \"zero\"], col = \"red\", on = 1)\n\npar(mfrow = c(2,1))\nplot(sp500prices, main = \"Daily prices: S&P500\")\ngraf_returns\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/datos-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculating annualized volatilities in years close to economic crises\nsd_annualized <- \n  as.data.frame(cbind(\"2008\" = sqrt(252)*sd(sp500ret[\"2008\"]), \n        \"2009\" = sqrt(252)*sd(sp500ret[\"2009\"]), \n        \"2017\" = sqrt(252)*sd(sp500ret[\"2017\"]), \n        \"2018\" = sqrt(252)*sd(sp500ret[\"2018\"]), \n        \"2019\" = sqrt(252)*sd(sp500ret[\"2019\"]), \n        \"2020\" = sqrt(252)*sd(sp500ret[\"2020\"]), \n        \"2021\" = sqrt(252)*sd(sp500ret[\"2021\"]),\n        \"total\"= sqrt(252)*sd(sp500ret, na.rm = T)))\n\nkable(sd_annualized, digits = 4, caption = \"Calculating annualized volatilities in years close to economic crises\", booktabs = T) %>% \n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Calculating annualized volatilities in years close to economic crises</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> 2008 </th>\n   <th style=\"text-align:right;\"> 2009 </th>\n   <th style=\"text-align:right;\"> 2017 </th>\n   <th style=\"text-align:right;\"> 2018 </th>\n   <th style=\"text-align:right;\"> 2019 </th>\n   <th style=\"text-align:right;\"> 2020 </th>\n   <th style=\"text-align:right;\"> 2021 </th>\n   <th style=\"text-align:right;\"> total </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.4097 </td>\n   <td style=\"text-align:right;\"> 0.2729 </td>\n   <td style=\"text-align:right;\"> 0.0669 </td>\n   <td style=\"text-align:right;\"> 0.1705 </td>\n   <td style=\"text-align:right;\"> 0.1247 </td>\n   <td style=\"text-align:right;\"> 0.3443 </td>\n   <td style=\"text-align:right;\"> 0.131 </td>\n   <td style=\"text-align:right;\"> 0.1798 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nVolatility fluctuations over different time periods can be observed by means of the 1-month and 3-month rolling volatility:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,1))\n\n# 1-month rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"1-month rolling volatility\")\n\n# 3-months rolling annualized volatility estimate\nchart.RollingPerformance(R = sp500ret[\"2000::2022\"], width = 3*22,\n                         FUN = \"sd.annualized\", scale = 252, main = \"3-month rolling volatility\")\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/Rolling volatility-1.png){width=672}\n:::\n:::\n\n\nAs can be seen, the volatility of S&P500 returns varies over time and may depend on past variance, so to study its dynamics a process is needed that allows the conditional variance to change over time as a function of past errors, in the manner of the autoregressive conditional heteroscedasticity (ARCH) model proposed by Engle (1982), and the subsequent generalization (GARCH) that allows for a longer memory in the model and a more flexible lag structure (Bollerslev, 1986).\n\nWhen developing conditional heteroscedasticity models, the specifications for the equations for the conditional mean, conditional variance and conditional error distribution should be kept in mind.\n\n### Specification of the conditional variance and conditional error distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specification of the Standard GARCH model\ngarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"sGARCH\"), \n                        distribution.model = \"norm\")\n\ngarchfit <- ugarchfit(data = sp500ret, spec = garchspec)\n\n# Obtain the estimated volatility \ngarchvol <- sigma(garchfit)\nplot(garchvol, main = \"Estimated Volatility\")\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/a simple model-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Calculate the unconditional volatility and standardized returns.\nkable(sqrt(uncvariance(garchfit)), digits = 4, booktabs = T,\n      col.names =\"Unconditional inflation volatility\")\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Unconditional inflation volatility </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0109 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visual comparison of error distribution\nstdret <- residuals(garchfit, standardize = TRUE)\n\nchart.Histogram(stdret, methods = c(\"add.normal\",\"add.density\"), \n                colorset = c(\"gray\",\"red\",\"blue\"), \n                main = \"Distribution of standardized returns vs. normal dist.\")\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/error distribution comparison-1.png){width=672}\n:::\n:::\n\n\nDue to the presence of extreme events, the empirical distribution of the returns have thick tails, so the normality assumption may be inadequate and may have generated biased estimators. Considering that the distribution of the standardized returns does not follow a normal distribution, as they have excess kurtosis, thicker tails and an asymmetric effect, setting a t-Student distribution with skewness instead of a normal distribution for the specification of the conditional error distribution leads to a more realistic GARCH model.\n\nOn the other hand, when there is a leverage effect, negative news about returns (negative shocks) affect the variance more than positive news (positive shocks). Because of the asymmetric effect of the distribution, a gjr-GARCH model is applied to confirm a possible leverage effect. In this model, an asymmetric variance response to positive and negative news is allowed for by assigning more weight on negative news by taking $(\\alpha + \\gamma)*\\varepsilon_t^2$ when $\\varepsilon_t \\le 0$ instead of only $\\alpha*\\varepsilon_t^2$ when $\\varepsilon_t > 0$. The news impact curve is a useful tool for visualizing the response of variance to surprise in returns.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# GJR-GARCH Specification\ngjrgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                           variance.model = list(model = \"gjrGARCH\"),\n                           distribution.model = \"sstd\")\n\ngjrgarchfit <- ugarchfit(data = sp500ret, spec = gjrgarchspec)\n\n# Obtain the estimated volatility \ngjrgarchvol_mean <- fitted(gjrgarchfit)\ngjrgarchvol <- sigma(gjrgarchfit)\n \n# Compare estimated volatilities\nplotvol <- plot(abs(sp500ret), col = \"grey\", \n                main = \"Estimated returns and volatilities\")\nplotvol <- addSeries(gjrgarchvol, col = \"red\", on=1)\nplotvol <- addSeries(garchvol, col = \"blue\", on=1)\nplotvol\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/GJR-Garch-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# News Curve\nplot(gjrgarchfit, which = 12)\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/News Curve-1.png){width=672}\n:::\n:::\n\n\nAn asymmetric leverage effect can be seen in the figure above. From it we conclude that the conditional variance reaction is greater to past negative shocks than to past positive shocks of the same magnitude in S&P500 returns. So much so, that the market is relatively insensitive to positive shocks compared to the increase and sensitivity of volatility that accompanies negative shocks with a further decline in prices.\n\n**Model selection based on information criteria**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# log-likelihood of both models\nlikelihood <- \n  data.frame(Garch = likelihood(garchfit), gjrGarch = likelihood(gjrgarchfit))\nrownames(likelihood) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria <- \n  data.frame(Garch = infocriteria(garchfit), gjrGarch = infocriteria(gjrgarchfit))\n\ncolnames(information_criteria) <- c(\"Garch\", \"gjrGarch\")\n\nkable(rbind(information_criteria, likelihood), booktabs = T,\n      col.names = c(\"Garch\", \"gjrGarch\"),\n      digits = 4,\n      caption = \"Information Criteria for Conditional Heteroscedasticity Models\") %>% \n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Information Criteria for Conditional Heteroscedasticity Models</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Garch </th>\n   <th style=\"text-align:right;\"> gjrGarch </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Akaike </td>\n   <td style=\"text-align:right;\"> -6.5656 </td>\n   <td style=\"text-align:right;\"> -6.6531 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bayes </td>\n   <td style=\"text-align:right;\"> -6.5623 </td>\n   <td style=\"text-align:right;\"> -6.6472 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shibata </td>\n   <td style=\"text-align:right;\"> -6.5656 </td>\n   <td style=\"text-align:right;\"> -6.6531 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hannan-Quinn </td>\n   <td style=\"text-align:right;\"> -6.5645 </td>\n   <td style=\"text-align:right;\"> -6.6511 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log-Likelihood </td>\n   <td style=\"text-align:right;\"> 27471.3350 </td>\n   <td style=\"text-align:right;\"> 27840.3342 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nBased on the information criteria, it is concluded that the GJR-GARCH model with t-Student distribution is a more adequate and realistic model than a simple model with normal distribution.\n\n### The mean model\n\nModeling the dynamics of the conditional mean generally has a large effect on estimated returns, but only a small effect on volatility predictions. Financial econometrics theory suggests that the effect can be so minimal, that if the interest is only in the volatility dynamics, one can generally ignore the mean dynamics and assume a simple constant mean specification to save parsimony. To confirm that this is the case, we will consider an AR(1) model and a GARCH model in mean to see how the volatilities are related.\n\nIn the AR(1) model, the sign of the autoregressive parameter depends on the market reaction to the news $\\mu_t = \\mu + \\rho(R_{t-1} - \\mu)$. A positive value of $\\rho$ is consistent with the interpretation that markets underreact to news, leading to momentum in returns (above-average returns are followed by above-average returns). A negative value of $|rho$ is consistent with the interpretation that markets overreact to news, leading to a reversal in returns (above-average returns are followed by below-average returns). In any case, if $||rho|<1$, then deviations of returns from their long-term mean ($\\mu$) are transitory.\n\nSo the question arises: are the daily returns of the S&P500 characterized by momentum or a reversal effect in its AR(1) dynamics?\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# AR(1) specification of an asymmetric GARCH\nargarchspec <- ugarchspec(mean.model = list(armaOrder = c(1,0)),\n                        variance.model = list(model = \"gjrGARCH\"),\n                        distribution.model = \"sstd\")\n\nargarchfit <- ugarchfit(data =  sp500ret, spec = garchspec)\n\n# Mean and volatility of the AR(1) model\nar1_mean <- fitted(argarchfit)\nar1_vol <- sigma(argarchfit)\n\n# Coefficients of the mean model\nkable(coef(argarchfit)[c(1:2)], digits = 6, booktabs = T, format.args = list(scientific = FALSE), col.names =\"Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional\") %>% \n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> Coeficientes del modelo GARCH con una especificación AR(1) en la media condicional </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> mu </td>\n   <td style=\"text-align:right;\"> 0.000642 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> omega </td>\n   <td style=\"text-align:right;\"> 0.000002 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nSince the AR(1) coefficient in the mean model is negative, we find a reversal effect in terms of predicted return. After an above-average return, we expect a below-average return. And after below-average return, we expect above-average return. Also, since this coefficient is close to zero, deviations from daily returns are transitory.\n\nIn contrast to the use of an ARMA model for the mean, we have that a GARCH model in mean does not make use of the autocorrelation of returns. Instead, it exploits the relationship between the expected return and the variance of the return. The higher the risk in terms of variance, the higher the expected return of the investment should be. So it is a model that quantifies the trade-off between risk and reward.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specification of a GARCH in mean\ngim_garchspec <- ugarchspec( \n  mean.model = list(armaOrder = c(0,0), archm = TRUE, archpow = 2),\n  variance.model = list(model = \"gjrGARCH\"), \n  distribution.model = \"sstd\")\n\ngim_garchfit <- ugarchfit(data = sp500ret , spec = gim_garchspec)\n\n# Mean and volatility of the AR(1) model\ngim_mean <- fitted(gim_garchfit)\ngim_vol <- sigma(gim_garchfit)\n\n# Correlation between the estimated returns of the AR(1) model and the average model\nkable(cor(ar1_mean, gim_mean), \n      col.names = \"Correlación entre los retornos estimados del modelo Garch-AR(1) y el modelo Garch en media\", digits = 4, booktabs = T) %>%\n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Correlación entre los retornos estimados del modelo Garch-AR(1) y el modelo Garch en media </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.4406 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Correlation between the estimated returns of the mean model and the AR(1) model\nmodel_correlation <- as.data.frame(cor(merge(gjrgarchvol, ar1_vol, gim_vol)))\nrownames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\ncolnames(model_correlation) <- c(\"gjrGarch\", \"AR(1)\", \"Garch in mean\")\n\nkable(model_correlation, booktabs = T,\n      digits = 4,\n      caption = \"Correlation between estimated volatilities for different specifications of Garch models\") %>% \n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Correlation between estimated volatilities for different specifications of Garch models</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> gjrGarch </th>\n   <th style=\"text-align:right;\"> AR(1) </th>\n   <th style=\"text-align:right;\"> Garch in mean </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> gjrGarch </td>\n   <td style=\"text-align:right;\"> 1.0000 </td>\n   <td style=\"text-align:right;\"> 0.9678 </td>\n   <td style=\"text-align:right;\"> 0.9993 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AR(1) </td>\n   <td style=\"text-align:right;\"> 0.9678 </td>\n   <td style=\"text-align:right;\"> 1.0000 </td>\n   <td style=\"text-align:right;\"> 0.9696 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Garch in mean </td>\n   <td style=\"text-align:right;\"> 0.9993 </td>\n   <td style=\"text-align:right;\"> 0.9696 </td>\n   <td style=\"text-align:right;\"> 1.0000 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThere is a large disagreement between the predicted returns obtained with the Garch-AR(1) and GARCH models on average, as evidenced by the low correlation. Because the mean return is close to zero for daily returns, these differences in mean prediction have little impact on volatility predictions. Their correlation is almost one. Since we are only interested in volatility dynamics, and to keep parsimony, we will estimate GARCH models with constant mean.\n\n### Extension of the analysis of the volatility dynamics of SP500 returns.\n\nIn the past graphs, we observed how financial return volatility is clustered over time: periods of above-average volatility are followed by periods of below-average volatility. In the long term, it is expected that:\n\n-   When volatility is high, volatility will decline and return to its long-term average.\n-   When volatility is low, volatility will increase and return to its long-term average.\n\nIn the estimation of GARCH models we can exploit this mean-reverting behavior of volatility by volatility targeting and confirm that the long-run volatility implied by the GARCH model is equal to the sample standard deviation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specification for variance targeting\ntargarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\ntargarchfit <- ugarchfit(data = sp500ret, spec = targarchspec)\n\ntargarchvol <- sigma(targarchfit)\n\n# Compare against gjrGarch\n\n# log-likelihood of the two models\nlikelihood_2 <- \n  data.frame(gjrGarch = likelihood(gjrgarchfit), tarGarch = likelihood(targarchfit))\nrownames(likelihood_2) <- \"Log-Likelihood\"\n\n# Information criteria\ninformation_criteria_2 <- \n  data.frame(gjrGarch = infocriteria(gjrgarchfit), tarGarch = infocriteria(targarchfit))\n\ncolnames(information_criteria_2) <- c(\"gjrGarch\", \"tarGarch\")\n\nkable(rbind(information_criteria_2, likelihood_2), booktabs = T,\n      col.names = c(\"gjrGarch\", \"Volatility targeting gjrGarch\"),\n      digits = 4,\n      caption = \"Information Criteria for GARCH models with volatility targeting\") %>% \n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Information Criteria for GARCH models with volatility targeting</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> gjrGarch </th>\n   <th style=\"text-align:right;\"> Volatility targeting gjrGarch </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Akaike </td>\n   <td style=\"text-align:right;\"> -6.6531 </td>\n   <td style=\"text-align:right;\"> -6.6532 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Bayes </td>\n   <td style=\"text-align:right;\"> -6.6472 </td>\n   <td style=\"text-align:right;\"> -6.6482 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Shibata </td>\n   <td style=\"text-align:right;\"> -6.6531 </td>\n   <td style=\"text-align:right;\"> -6.6532 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Hannan-Quinn </td>\n   <td style=\"text-align:right;\"> -6.6511 </td>\n   <td style=\"text-align:right;\"> -6.6515 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Log-Likelihood </td>\n   <td style=\"text-align:right;\"> 27840.3342 </td>\n   <td style=\"text-align:right;\"> 27839.8445 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n# Implied long term volatility\nkable(data.frame(Garch = sqrt(uncvariance(garchfit)), \n                 tarGarch = sqrt(uncvariance(targarchfit))), \n      col.names = c(\"Simple GARCH model\", \"GJR-Garch model with volatility targeting\"), \n      digits = 4, booktabs = T,\n      caption = \"Implied long-term volatility\") %>%\n  kable_styling(full_width = T)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Implied long-term volatility</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> Simple GARCH model </th>\n   <th style=\"text-align:right;\"> GJR-Garch model with volatility targeting </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.0109 </td>\n   <td style=\"text-align:right;\"> 0.0113 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe models that have been studied so far lead to an in-sample volatility estimate obtained by estimating the GARCH model only once and using the full time series, which can cause bias (look-ahead bias). However, in moving window models these biases are avoided by conditioning the estimation to use only the returns available at the previous time of estimation. That is, the model would be re-estimated in each window using only the yields that are actually observable at the time of estimation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specification for garch on movable windows\nrollgarchspec <- ugarchspec(mean.model = list(armaOrder = c(0,0)),\n                        variance.model = list(model = \"gjrGARCH\",\n                                              variance.targeting = TRUE),\n                        distribution.model = \"sstd\")\n\nrollgarchfit <- ugarchroll(garchspec, \n                           data = sp500ret,\n                           n.start = 2000, refit.window = \"moving\",  refit.every = 500)\n\n# Mobile predictions\npreds <- as.data.frame(rollgarchfit)\n\n# Comparison of the estimated volatility in the in-sample model and the moving model\ngarchvolroll <- xts(preds$Sigma, order.by = as.Date(rownames(preds)))\nvolplot <- plot(gjrgarchvol, col = \"darkgrey\", lwd = 1.5, \n                main = \"In-sample and moving window volatility forecasts\")\nvolplot <- addSeries(garchvolroll, col = \"blue\", on = 1)\nplot(volplot)\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nTactical Asset Allocation using Target Volatility\\*\\* \\*\\*Targeted Volatility\n\nGARCH volatility predictions have a direct practical use in portfolio allocation. According to James Tobin's two-fund separation theorem, you should invest a proportion w of your wealth in a risky portfolio and the remainder in a risk-free asset, such as a U.S. Treasury bill. If you target a portfolio with an annualized volatility of 5%, and the annualized volatility of the risky asset is w, then you should invest (0.05/w) in the risky asset, in this case, the S&P500.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Annualized volatility from gjrGARCH moving window model with \n# asymmetric t-Student distribution\nestimated_annualized_volatility <- sqrt(252)*garchvolroll\n\n# Calculate the weights assigned to the risky asset with a target volatility of 5% per year. \nweight <- 0.05 / estimated_annualized_volatility\n\n# Compare the annualized volatility with the portfolio's weights\nplot(merge(estimated_annualized_volatility, weight), multi.panel = TRUE,\n     main = \"Annualized vol. and the risk asset allocation weights\")\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nIn the above chart it is easy to observe the dynamics of asset allocation within a portfolio. In our portfolio composed of U.S. treasury bills and the S&P500, since treasury bills are risk-free, the question that arises is what weight should be assigned to the S&P500 within the portfolio given the risk exposure constraint we impose on ourselves. By March 2022, the model recommends a portfolio with approximately 20% exposure to the S&P500 and 80% in risk-free assets with a target volatility of 5%.\n\n**Brief application of a VaR - Value at Risk**.\n\nThe value-at-risk charts show substantial temporal variation in downside risk. This time variation is primarily due to the time variation in volatility.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 5% Value at Risk\ngarchVaR <- quantile(rollgarchfit, probs = 0.05)\n\n# Volatility for Value at Risk\ngarchvolroll_var <- xts(preds$Sigma, order.by = time(garchVaR))\n\n# Visual analysis of movements\ngarchplot <- plot(garchvolroll_var, ylim = c(-0.1, 0.1), \n     main = \"Daily volatility and 5% VaR\")\ngarchplot <- addSeries(garchVaR, on = 1, col = \"blue\")\nplot(garchplot, \n     main = \"Daily volatility and 5% VaR\")\n```\n\n::: {.cell-output-display}\n![](2022-08-30_volatility_financial_asset_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe high co-movement between the two series is notorious. The intuition is that, if volatility soars, there is a risk of losing more money. That is why the value at risk also becomes more extreme. In March 2022, the value at risk of this portfolio is 1.95%, or almost \\$20 for every \\$1,000 invested in the risky asset.\n\nUsing the weights assigned in the previous exercise, if one were to invest \\$100,000 today: \\$20,000 would be invested in the S&P500 and \\$80,000 in U.S. Treasury bills. In aggregate, the portfolio would have a Value at Risk of \\$390.5. An amount exposed to risk of only 0.391% of invested capital.\n",
    "supporting": [
      "2022-08-30_volatility_financial_asset_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}